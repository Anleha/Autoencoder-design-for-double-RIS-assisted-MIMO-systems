{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam,RMSprop\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "# import cv2\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Autoencoder structure for the wireless sytem model\n",
    "class CNN_autoencoder(nn.Module):\n",
    "    def __init__(self, N_bit, Nt, Nr, N_RIS1, N_RIS2, Block_length):\n",
    "        super(CNN_autoencoder, self).__init__()\n",
    "        \n",
    "        self.N_bit = N_bit\n",
    "        self.Nt = Nt\n",
    "        self.Nr = Nr\n",
    "        self.N_RIS1 = N_RIS1\n",
    "        self.N_RIS2 = N_RIS2\n",
    "        kernel = 1\n",
    "        self.Block_length = Block_length\n",
    "        # sigma = 1e-12\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.N_bit,out_channels=256,kernel_size=kernel,stride=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(in_channels=256,out_channels=256,kernel_size=kernel,stride=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            \n",
    "            nn.Conv1d(in_channels=256,out_channels=2*Nt,kernel_size=kernel,stride=1),\n",
    "            nn.BatchNorm1d(2*Nt)\n",
    "        )\n",
    "     \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=2*self.Nr+2*self.Nt*self.Nr,out_channels=512,kernel_size=kernel,stride=1),\n",
    "#             nn.Conv1d(in_channels=2*self.Nr,out_channels=256,kernel_size=1,stride=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(in_channels=512,out_channels=512,kernel_size=1,stride=kernel),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels=512,out_channels=self.N_bit,kernel_size=kernel,stride=1),\n",
    "            #nn.BatchNorm1d(N_bit*Nt)\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "        self.RIS1 = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=2*self.Nt*N_RIS+2*N_RIS,out_channels=256,kernel_size=1,stride=1),\n",
    "            nn.Conv1d(in_channels=2*N_RIS1,out_channels=256,kernel_size=kernel,stride=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(in_channels=256,out_channels=256,kernel_size=kernel,stride=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels=256,out_channels=self.N_RIS1,kernel_size=kernel,stride=1),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "        self.RIS2 = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=2*self.Nt*N_RIS+2*N_RIS,out_channels=256,kernel_size=1,stride=1),\n",
    "            nn.Conv1d(in_channels=2*N_RIS2,out_channels=256,kernel_size=kernel,stride=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(in_channels=256,out_channels=256,kernel_size=kernel,stride=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels=256,out_channels=self.N_RIS2,kernel_size=kernel,stride=1),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode_signal(self, x):\n",
    "        x=self.encoder(x)\n",
    "        return x\n",
    "\n",
    "    def decode_signal(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def AWGN(self, x, h1_tensor, h2_tensor, D_tensor, g1_tensor, g2_tensor, phase1, phase2 ,SNR, device):\n",
    "        # x = self.Nt*(x / x.norm(dim=-1)[:, None])\n",
    "        SNR_linear = 10.0**(SNR/10.0)\n",
    "        batch = x.shape[0]\n",
    "        scale = 1e3\n",
    "    \n",
    "        x_compl = torch.zeros([batch,self.Nt,self.Block_length],dtype=torch.cfloat,device=device)\n",
    "        for i in range(self.Nt):\n",
    "            x_compl[:,i] = torch.complex(x[:,2*i],x[:,2*i+1])\n",
    "            \n",
    "        encode_out_compl_tensor = torch.permute(x_compl,(0,2,1))\n",
    "        encode_out_compl_tensor = torch.reshape(encode_out_compl_tensor,(batch*Block_length,Nt))   \n",
    "        #calculate cascaded channel    \n",
    "        theta1 = torch.exp(1j*2*math.pi*phase1)\n",
    "        theta2 = torch.exp(1j*2*math.pi*phase2)\n",
    "        \n",
    "        theta1_tensor = torch.permute(theta1,(0,2,1))\n",
    "        theta1_tensor = torch.reshape(theta1_tensor,(batch*Block_length,N_RIS1))\n",
    "        \n",
    "        theta2_tensor = torch.permute(theta2,(0,2,1))\n",
    "        theta2_tensor = torch.reshape(theta2_tensor,(batch*Block_length,N_RIS2))\n",
    "        \n",
    "        #calculate channel\n",
    "        temp = torch.matmul(torch.diag_embed(theta1_tensor),h1_tensor)\n",
    "        H1phi1D_tensor = torch.matmul(D_tensor,temp)\n",
    "        \n",
    "        temp = torch.matmul(torch.diag_embed(theta2_tensor),H1phi1D_tensor)\n",
    "        doublelink_tensor = torch.matmul(g2_tensor,temp)\n",
    "        \n",
    "        temp = torch.matmul(torch.diag_embed(theta1_tensor),h1_tensor)\n",
    "        H1phi1G1_tensor = torch.matmul(g1_tensor,temp)\n",
    "        \n",
    "        temp = torch.matmul(torch.diag_embed(theta2_tensor),h2_tensor)\n",
    "        H2phi2G2_tensor = torch.matmul(g2_tensor,temp)\n",
    "        \n",
    "        cascaded_channel_tensor = doublelink_tensor*scale**2 + H1phi1G1_tensor*scale**2 + H2phi2G2_tensor*scale**2\n",
    "        \n",
    "        #transmit data via RIS\n",
    "        y_compl_tensor = torch.matmul(cascaded_channel_tensor,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        temp = torch.reshape(torch.view_as_real(y_compl_tensor),(batch*Block_length,2*self.Nr))\n",
    "        temp = torch.reshape(temp,(batch,Block_length,2*Nr))\n",
    "        decode_input = torch.permute(temp,(0,2,1))\n",
    "        \n",
    "        noise = torch.randn(*decode_input.size(),device=device) / ((SNR_linear) ** 0.5)\n",
    "\n",
    "        x = decode_input + noise\n",
    "        \n",
    "        return x, y_compl_tensor\n",
    "    def forward(self, x, h1, h2, D, g1, g2, EbNo_train):\n",
    "#         EbNo_train = 1\n",
    "        scale = 1e3\n",
    "        device=torch.device('cuda')\n",
    "        batch = x.shape[0]\n",
    "        \n",
    "        T1 = timeit.default_timer()\n",
    "        #change data to tensor \n",
    "        h1_tensor = torch.permute(h1,(0,3,1,2))\n",
    "        h1_tensor = torch.reshape(h1_tensor,(batch*Block_length,N_RIS1,Nt))\n",
    "        \n",
    "        h2_tensor = torch.permute(h2,(0,3,1,2))\n",
    "        h2_tensor = torch.reshape(h2_tensor,(batch*Block_length,N_RIS2,Nt))\n",
    "        \n",
    "        D_tensor = torch.permute(D,(0,3,1,2))\n",
    "        D_tensor = torch.reshape(D_tensor,(batch*Block_length,N_RIS2,N_RIS1))\n",
    "        \n",
    "        g1_tensor = torch.permute(g1,(0,3,1,2))\n",
    "        g1_tensor = torch.reshape(g1_tensor,(batch*Block_length,Nr,N_RIS1))\n",
    "        \n",
    "        g2_tensor = torch.permute(g2,(0,3,1,2))\n",
    "        g2_tensor = torch.reshape(g2_tensor,(batch*Block_length,Nr,N_RIS2))\n",
    "        \n",
    "        T2 = timeit.default_timer()\n",
    "        #input for encoder\n",
    "        # encode_in = torch.cat((x,cascaded_channel_real),dim=1)\n",
    "        \n",
    "        encode_in = x\n",
    "        \n",
    "        encode_out = self.encoder(encode_in)\n",
    "        #power normalization\n",
    "        mean = (encode_out**2).mean()\n",
    "        encode_out = encode_out/torch.sqrt(2*Nt*mean)\n",
    "        \n",
    "        #convert encode_out to complex\n",
    "        encode_out_compl = torch.zeros([batch,Nt,Block_length],dtype=torch.cfloat,device=device)\n",
    "        for i in range(self.Nt):\n",
    "            encode_out_compl[:,i,:] = torch.complex(encode_out[:,2*i,:],encode_out[:,2*i+1,:])\n",
    "        #input for RIS1\n",
    "        encode_out_compl_tensor = torch.permute(encode_out_compl,(0,2,1))\n",
    "        encode_out_compl_tensor = torch.reshape(encode_out_compl_tensor,(batch*Block_length,Nt))\n",
    "\n",
    "        y_ris1 = torch.matmul(h1_tensor,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "        y_ris1 = y_ris1*scale\n",
    "        temp = torch.reshape(torch.view_as_real(y_ris1),(batch*Block_length,2*N_RIS1))\n",
    "        temp = torch.reshape(temp,(batch,Block_length,2*N_RIS1))\n",
    "        y_ris1_real = torch.permute(temp,(0,2,1))\n",
    "            \n",
    "        phase1 = self.RIS1(y_ris1_real)\n",
    "        theta1 = torch.exp(1j*2*math.pi*phase1)\n",
    "        \n",
    "        theta1_tensor = torch.permute(theta1,(0,2,1))\n",
    "        theta1_tensor = torch.reshape(theta1_tensor,(batch*Block_length,N_RIS1))\n",
    "        T3 = timeit.default_timer()\n",
    "        \n",
    "        #input for RIS2\n",
    "        temp = torch.matmul(torch.diag_embed(theta1_tensor),h1_tensor)\n",
    "        H1phi1D_tensor = torch.matmul(D_tensor,temp)\n",
    "        # y1_ris2 = torch.matmul(H1phi1D_tensor,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "        # y2_ris2 = torch.matmul(h2_tensor,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "        y_ris2 = torch.matmul(H1phi1D_tensor*scale+h2_tensor*scale,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "        # y_ris2 = y1_ris2 + y2_ris2\n",
    "        \n",
    "        temp = torch.reshape(torch.view_as_real(y_ris2),(batch*Block_length,2*self.N_RIS2))\n",
    "        temp = torch.reshape(temp,(batch,Block_length,2*N_RIS2))\n",
    "        y_ris2_real = torch.permute(temp,(0,2,1))\n",
    "      \n",
    "        phase2 = self.RIS2(y_ris2_real)\n",
    "        theta2 = torch.exp(1j*2*math.pi*phase2)\n",
    "      \n",
    "        theta2_tensor = torch.permute(theta2,(0,2,1))\n",
    "        theta2_tensor = torch.reshape(theta2_tensor,(batch*Block_length,N_RIS2))\n",
    "        T4 = timeit.default_timer()\n",
    "        \n",
    "        #calculate cascaded channel\n",
    "        temp = torch.matmul(torch.diag_embed(theta2_tensor),H1phi1D_tensor)\n",
    "        doublelink_tensor = torch.matmul(g2_tensor,temp)\n",
    "        \n",
    "        temp = torch.matmul(torch.diag_embed(theta1_tensor),h1_tensor)\n",
    "        H1phi1G1_tensor = torch.matmul(g1_tensor,temp)\n",
    "        \n",
    "        temp = torch.matmul(torch.diag_embed(theta2_tensor),h2_tensor)\n",
    "        H2phi2G2_tensor = torch.matmul(g2_tensor,temp)\n",
    "        \n",
    "#         cascaded_channel_tensor = doublelink_tensor*scale**3 + H1phi1G1_tensor*scale**2  + H2phi2G2_tensor*scale**2 \n",
    "        cascaded_channel_tensor = doublelink_tensor*scale**2 + H1phi1G1_tensor*scale**2  + H2phi2G2_tensor*scale**2\n",
    "#         print(cascaded_channel_tensor)\n",
    "#         print(g2_tensor)\n",
    "        temp = torch.reshape(torch.view_as_real(cascaded_channel_tensor),(batch*Block_length,2*Nr*Nt))\n",
    "        temp = torch.reshape(temp,(batch,Block_length,2*Nr*Nt))\n",
    "        cascaded_channel_real = torch.permute(temp,(0,2,1))\n",
    "        # cascaded_channel = torch.reshape(cascaded_channel_tensor,(batch,Block_length,Nr,Nt))\n",
    "        # cascaded_channel = torch.permute(cascaded_channel,(0,2,3,1))\n",
    "        T5 = timeit.default_timer()\n",
    "#         print(cascaded_channel_tensor)\n",
    "        #received signal\n",
    "        y_compl_tensor = torch.matmul(cascaded_channel_tensor,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "#         print(y_compl_tensor)\n",
    "     \n",
    "#         print(y_compl_tensor)\n",
    "        temp = torch.reshape(torch.view_as_real(y_compl_tensor),(batch*Block_length,2*self.Nr))\n",
    "        temp = torch.reshape(temp,(batch,Block_length,2*Nr))\n",
    "        decode_input = torch.permute(temp,(0,2,1))\n",
    "        \n",
    "        T6 = timeit.default_timer()\n",
    "#         received_power = sum(torch.linalg.norm(y_compl_tensor,dim=1))\n",
    "        \n",
    "        #AWGN noise\n",
    "        noise = Variable(torch.randn(*decode_input.size(),device=device) / ((2*(EbNo_train))**0.5))\n",
    "        decode_input = decode_input + noise\n",
    "        mean = (decode_input**2).mean()\n",
    "#         decode_input = decode_input/torch.sqrt(2*mean)\n",
    "#         channel_mean = (cascaded_channel_real**2).mean()\n",
    "#         cascaded_channel_real = cascaded_channel_real/torch.sqrt(2*channel_mean)\n",
    "        decode_input = torch.cat((decode_input,cascaded_channel_real),dim=1)\n",
    "        decode_out = self.decoder(decode_input)\n",
    "        \n",
    "        T7 = timeit.default_timer()\n",
    "        \n",
    "\n",
    "        return decode_out, y_compl_tensor, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Rice Correlated channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement channel model for the wireless system\n",
    "\n",
    "def functionDoubleScatter(NumOfTransmitAnten,NumOfReceiveAnten,NumOfScatter,alpha):\n",
    "    beta= 1 #PathLoss factor\n",
    "    dl = 0.5\n",
    "    dS = 1\n",
    "    R = 1\n",
    "#     alpha = 0\n",
    "    \n",
    "    angularSpread = 2*math.pi/3\n",
    "    theta_tilda = 0.4595\n",
    "    R1 = torch.zeros(NumOfTransmitAnten,NumOfTransmitAnten,dtype=torch.cfloat) #correlation matrix between BS and scatter\n",
    "    R_tilda = torch.zeros(NumOfScatter,NumOfScatter,dtype=torch.cfloat) #correlation matrix\n",
    "    R2 = torch.zeros(NumOfReceiveAnten,NumOfReceiveAnten,dtype=torch.cfloat)\n",
    "    \n",
    "    ii = torch.arange(1-NumOfScatter,NumOfScatter,1)\n",
    "    theta_array = ii*angularSpread/(NumOfScatter-1)\n",
    "    theta_tilda_array = ii*2*theta_tilda/(NumOfScatter-1)\n",
    "    \n",
    "    for i in range(NumOfTransmitAnten):\n",
    "        for j in range(NumOfTransmitAnten):\n",
    "            temp = 0\n",
    "            for n in range(NumOfScatter):\n",
    "                temp += torch.exp(-torch.pi*2*1j*(i-j)*dl*torch.cos(torch.pi/2+alpha+theta_array[n]))\n",
    "            \n",
    "            R1[i,j] = 1/NumOfScatter*temp\n",
    "    \n",
    "    for i in range(NumOfScatter):\n",
    "        for j in range(NumOfScatter):\n",
    "            temp_tilda = 0\n",
    "            for n in range(NumOfScatter):\n",
    "                temp_tilda += torch.exp(-torch.pi*2*1j*(i-j)*dS*torch.cos(torch.pi/2+theta_tilda_array+theta_array[n]))\n",
    "            \n",
    "            R_tilda[i,j] = 1/NumOfScatter*temp\n",
    "            \n",
    "    for i in range(NumOfReceiveAnten):\n",
    "        for j in range(NumOfReceiveAnten):\n",
    "            temp = 0\n",
    "            for n in range(NumOfScatter):\n",
    "                temp += torch.exp(-torch.pi*2*1j*(i-j)*dl*torch.cos(torch.pi/2+alpha+theta_array[n]))\n",
    "            R2[i,j] = 1/NumOfScatter*temp\n",
    "    return R1, R_tilda, R2\n",
    "\n",
    "def functionScatterChannelGen(NumOfTransmitAnten,NumOfReceiveAnten,NumOfScatter,R1,R_tilda,R2,gen_num):\n",
    "    beta = 1\n",
    "    R1_square = sqrtm(R1)\n",
    "    temp = torch.tensor(R1_square,dtype=torch.cfloat).unsqueeze(0)\n",
    "    R1_square = temp.repeat(gen_num,1,1)\n",
    "    \n",
    "    R_tilda_square = sqrtm(R_tilda)\n",
    "    temp = torch.tensor(R_tilda_square,dtype=torch.cfloat).unsqueeze(0)\n",
    "    R_tilda_square = temp.repeat(gen_num,1,1)\n",
    "    \n",
    "    R2_square = sqrtm(R2)\n",
    "    temp = torch.tensor(R2_square,dtype=torch.cfloat).unsqueeze(0)\n",
    "    R2_square = temp.repeat(gen_num,1,1)\n",
    "    \n",
    "    G = torch.randn([gen_num,NumOfScatter,NumOfTransmitAnten],dtype=torch.cfloat)\n",
    "    g = torch.randn([gen_num,NumOfReceiveAnten,NumOfScatter],dtype=torch.cfloat)\n",
    "    \n",
    "    channel = torch.matmul(R2_square,g)\n",
    "    channel = torch.matmul(channel,R_tilda_square)\n",
    "    \n",
    "    channel = torch.matmul(channel,G)\n",
    "    channel = torch.matmul(channel,R1_square)\n",
    "    channel = torch.sqrt(torch.tensor(beta/NumOfScatter))*channel\n",
    "    \n",
    "    return channel\n",
    "\n",
    "def array_response_h(theta,phi,Ah,d_labmda):\n",
    "    i = torch.arange(0,Ah,1)\n",
    "    return torch.exp(1j*2*torch.pi*d_lambda*math.cos(theta)*math.sin(phi)*i)\n",
    "\n",
    "def array_response_v(theta,Av,d_lambda):\n",
    "    i = torch.arange(0,Av,1)\n",
    "    \n",
    "    return torch.exp(1j*2*torch.pi*d_lambda*math.sin(theta)*i)\n",
    "\n",
    "def UPA_response(theta,phi,Av,Ah,d_lambda):\n",
    "    a_v = array_response_v(theta,Av,d_lambda)\n",
    "    a_h = array_response_h(theta,phi,Ah,d_lambda)\n",
    "    \n",
    "    return torch.kron(a_v,a_h)\n",
    "\n",
    "def ULA_response(theta,N,d_lamda):\n",
    "    i = torch.arange(0,N,1)\n",
    "    \n",
    "    return torch.exp(1j*2*torch.pi*d_lambda*math.sin(theta)*i)\n",
    "\n",
    "def correlated_channel(N_t,N_r,rho_t,rho_r,gen_num):\n",
    "    h = torch.randn([gen_num,N_r,N_t],dtype=torch.cfloat)\n",
    "    correlated_channel = torch.zeros(gen_num,N_r,N_t,dtype=torch.cfloat)\n",
    "    R1 = torch.zeros(N_r,N_r)\n",
    "    R2 = torch.zeros(N_t,N_t)\n",
    "    for i in range(N_r):\n",
    "        for j in range(N_r):\n",
    "            R1[i,j] = pow(rho_r,abs(i-j))\n",
    "    for i in range(N_t):\n",
    "        for j in range(N_t):\n",
    "            R2[i,j] = pow(rho_t,abs(i-j))\n",
    "    R1_square = sqrtm(R1)\n",
    "    temp = torch.tensor(R1_square,dtype=torch.cfloat).unsqueeze(0)\n",
    "    R1_square = temp.repeat(gen_num,1,1)\n",
    "    R2_square = sqrtm(R2)\n",
    "    temp = torch.tensor(R2_square,dtype=torch.cfloat).unsqueeze(0)\n",
    "    R2_square = temp.repeat(gen_num,1,1)\n",
    "    \n",
    "    temp = torch.matmul(R1_square,h)\n",
    "    correlated_channel = torch.matmul(temp,R2_square)\n",
    "    return correlated_channel\n",
    "\n",
    "def Rice_channel(K,theta_A,phi_A,Nr_v,Nr_h,Nr,rho_r,theta_D,phi_D,Nt_v,Nt_h,Nt,rho_t,d_lambda,gen_num,d,num_scatter,alpha,mode):\n",
    "    sigma = 1e-12\n",
    "#     H_tilda = correlated_channel(Nt,Nr,rho_t,rho_r,gen_num) #correlated channel\n",
    "    R1, R_tilda, R2 = functionDoubleScatter(Nt,Nr,num_scatter,alpha)\n",
    "    H_tilda = functionScatterChannelGen(Nt,Nr,num_scatter,R1,R_tilda,R2,gen_num) #double_scatter channel\n",
    "    if mode == 'D':\n",
    "        H_bar = torch.matmul(torch.transpose(torch.conj(UPA_response(theta_A,phi_A,Nr_v,Nr_h,d_lambda).unsqueeze(0)),0,1),UPA_response(theta_D,phi_D,Nt_v,Nt_h,d_lambda).unsqueeze(0))\n",
    "    elif mode == 'H':\n",
    "        H_bar = torch.matmul(torch.transpose(torch.conj(UPA_response(theta_A,phi_A,Nr_v,Nr_h,d_lambda).unsqueeze(0)),0,1),ULA_response(theta_D,Nt,d_lambda).unsqueeze(0))\n",
    "    elif mode == 'G':\n",
    "        H_bar = torch.matmul(torch.transpose(torch.conj(ULA_response(theta_A,Nr,d_lambda).unsqueeze(0)),0,1),UPA_response(theta_D,phi_D,Nt_v,Nt_h,d_lambda).unsqueeze(0))\n",
    "    temp = H_bar.unsqueeze(0)\n",
    "    H_bar = temp.repeat(gen_num,1,1)\n",
    "    # F = 4*torch.randn(1)\n",
    "    \n",
    "#     beta_db = -34.53 -38*math.log10(d);\n",
    "    beta_db = -35.6 -22*math.log10(d);\n",
    "    beta = pow(10,beta_db/10);\n",
    "#     beta = beta/(sigma)\n",
    "    H = math.sqrt(beta)*(math.sqrt(K/(K+1))*H_bar + math.sqrt(1/(K+1))*H_tilda)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 100 #BS --> RIS distance\n",
    "\n",
    "d2 = 200 #BS --> user distance\n",
    "\n",
    "d = math.sqrt(d1**2+d2**2) #diag distance\n",
    "h = 2;\n",
    "# d3 = 25 #receiver --> BS horizontal distance\n",
    "\n",
    "# d_RIS_user = math.sqrt((d0-d)**2+dv**2)\n",
    "\n",
    "alpha = 3.5 #pathloss exponent\n",
    "\n",
    "Av = 8 #RIS vertical size\n",
    "\n",
    "Ah = 4 #RIS horizontal size\n",
    "\n",
    "d_lambda = 0.5 #d:lambda ratio\n",
    "\n",
    "#AoA/AoD angles\n",
    "#BS --> RIS1\n",
    "theta_A_TR1 = torch.pi/4\n",
    "theta_D_TR1 = torch.pi/2\n",
    "phi_A_TR1 = 0\n",
    "\n",
    "#BS to RIS2\n",
    "theta_A_TR2 = abs(torch.pi/4 - math.atan(d2/d1))\n",
    "theta_D_TR2 = math.atan(d1/d2)\n",
    "phi_A_TR2 = 0\n",
    "\n",
    "#RIS1 --> RIS2\n",
    "\n",
    "theta_A_R1R2 = torch.pi/4\n",
    "theta_D_R1R2 = torch.pi/4\n",
    "phi_A_R1R2 = 0\n",
    "phi_D_R1R2 = 0\n",
    "\n",
    "#RIS1 --> user\n",
    "theta_A_R1R = math.atan(d2/d1)\n",
    "theta_D_R1R = math.atan(d1/d2)\n",
    "phi_D_R1R = math.atan(h/math.sqrt(d1**2+d2**2))\n",
    "\n",
    "\n",
    "#RIS2 --> user\n",
    "theta_A_R2R = 0 \n",
    "theta_D_R2R = torch.pi/4\n",
    "phi_D_R2R = math.atan(h/d1)\n",
    "\n",
    "\n",
    "#RIS1 size\n",
    "K1_v = 4;\n",
    "K1_h = 8;\n",
    "K1 = K1_v*K1_h;\n",
    "N_RIS1 = K1;\n",
    "\n",
    "#RIS2 size\n",
    "K2_v = 4;\n",
    "K2_h = 8;\n",
    "K2 = K2_v*K2_h;\n",
    "N_RIS2 = K2;\n",
    "\n",
    "#BS antenna size\n",
    "Nt_v = 4\n",
    "Nt_h = 4\n",
    "Nt = Nt_v*Nt_h;\n",
    "\n",
    "#user antenna size\n",
    "Nr_v = 4\n",
    "Nr_h = 4\n",
    "Nr = Nr_v*Nr_h;\n",
    "\n",
    "#Rice parameters\n",
    "K = 0.2\n",
    "\n",
    "rho_t = 0.6\n",
    "rho_r = 0.6\n",
    "rho_RIS = 0.6\n",
    "\n",
    "num_scatter = 3\n",
    "#level of modulation\n",
    "N_bit = 64\n",
    "sigma = 1e-12\n",
    "gen_num = 10000\n",
    "Block_length = 20\n",
    "train_ratio = 0.9\n",
    "train_num = int(train_ratio*(gen_num/Block_length))\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate channel samples\n",
    "\n",
    "h1_init = Rice_channel(K,theta_A_TR1,phi_A_TR1,K1_v,K1_h,N_RIS1,rho_RIS,theta_D_TR1,0,Nt_v,Nt_h,Nt,rho_t,d_lambda,gen_num,d1,num_scatter,0,'H')\n",
    "h2_init = Rice_channel(K,theta_A_TR2,phi_A_TR2,K2_v,K2_h,N_RIS2,rho_RIS,theta_D_TR2,0,Nt_v,Nt_h,Nt,rho_t,d_lambda,gen_num,d,num_scatter,0,'H')\n",
    "D_init = Rice_channel(K,theta_A_R1R2,phi_A_R1R2,K2_v,K2_h,N_RIS2,rho_RIS,theta_D_R1R2,phi_D_R1R2,K1_v,K1_h,N_RIS1,rho_t,d_lambda,gen_num,d2,num_scatter,0,'D')\n",
    "g1_init = Rice_channel(K,theta_A_R1R,0,Nr_v,Nr_h,Nr,rho_r,theta_D_R1R,phi_D_R1R,K1_v,K1_h,N_RIS1,rho_t,d_lambda,gen_num,d,num_scatter,torch.pi/12,'G')\n",
    "g2_init = Rice_channel(K,theta_A_R2R,0,Nr_v,Nr_h,Nr,rho_r,theta_D_R2R,phi_D_R2R,K2_v,K2_h,N_RIS2,rho_t,d_lambda,gen_num,d1,num_scatter,torch.pi/12,'G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = int(gen_num/Block_length)\n",
    "\n",
    "temp = torch.reshape(h1_init,(batch,Block_length,N_RIS1,Nt))\n",
    "h1_all = torch.permute(temp,(0,2,3,1))\n",
    "\n",
    "temp = torch.reshape(h2_init,(batch,Block_length,N_RIS2,Nt))\n",
    "h2_all = torch.permute(temp,(0,2,3,1))\n",
    "\n",
    "temp = torch.reshape(D_init,(batch,Block_length,N_RIS2,N_RIS1))\n",
    "D_all = torch.permute(temp,(0,2,3,1))\n",
    "\n",
    "temp = torch.reshape(g1_init,(batch,Block_length,Nr,N_RIS1))\n",
    "g1_all = torch.permute(temp,(0,2,3,1))\n",
    "\n",
    "temp = torch.reshape(g2_init,(batch,Block_length,Nr,N_RIS2))\n",
    "g2_all = torch.permute(temp,(0,2,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = int(gen_num/Block_length*train_ratio)\n",
    "repeat_num = 2\n",
    "\n",
    "temp = h1_all[:batch_train,:,:,:Block_length]\n",
    "temp = temp.repeat(repeat_num,1,1,1)\n",
    "inx = torch.randperm(train_num)\n",
    "h1_train = temp[inx,:]\n",
    "\n",
    "temp = h2_all[:batch_train,:,:,:Block_length]\n",
    "temp = temp.repeat(repeat_num,1,1,1)\n",
    "inx = torch.randperm(train_num)\n",
    "h2_train = temp[inx,:]\n",
    "\n",
    "temp = D_all[:batch_train,:,:,:Block_length]\n",
    "temp = temp.repeat(repeat_num,1,1,1)\n",
    "inx = torch.randperm(train_num)\n",
    "D_train = temp[inx,:]\n",
    "\n",
    "temp = g1_all[:batch_train,:,:,:Block_length]\n",
    "temp = temp.repeat(repeat_num,1,1,1)\n",
    "inx = torch.randperm(train_num)\n",
    "g1_train = temp[inx,:]\n",
    "\n",
    "temp = g2_all[:batch_train,:,:,:Block_length]\n",
    "temp = temp.repeat(repeat_num,1,1,1)\n",
    "inx = torch.randperm(train_num)\n",
    "g2_train = temp[inx,:]\n",
    "\n",
    "del inx\n",
    "del temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training sample\n",
    "train_label = (torch.rand([train_num,Block_length]) * N_bit).long()\n",
    "train_data = torch.zeros(train_num,N_bit,Block_length)\n",
    "for i in range(train_num):\n",
    "    train_data[i,:] = torch.transpose(torch.sparse.torch.eye(N_bit).index_select(dim=0, index=train_label[i,:]),0,1)\n",
    "    \n",
    "dataset = Data.TensorDataset(train_data,train_label, h1_train, h2_train, D_train, g1_train, g2_train)\n",
    "train_loader = Data.DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()==True else 'cpu')\n",
    "model = CNN_autoencoder(N_bit, Nt, Nr, N_RIS1, N_RIS2, Block_length).to(device)\n",
    "#model training parameters\n",
    "optimizer = Adam(model.parameters(),lr=0.001)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Autoencoder_BLength20_16QAM_Rice1_Scatter3_MIMO16x16_train0dB.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(model.parameters(),lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "num_epoch = 100\n",
    "model = model.to(device)\n",
    "SNR_train = 20#db\n",
    "SNR_linear = 10**(SNR_train/10) #linear\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    for step, (data_in,label,h1,h2,D,g1,g2), in enumerate(train_loader): \n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            batch = data_in.shape[0]\n",
    "            data_in = data_in.to(device)\n",
    "            h1 = h1.to(device)\n",
    "            h2 = h2.to(device)\n",
    "            D = D.to(device)\n",
    "            g1 = g1.to(device)\n",
    "            g2 = g2.to(device)\n",
    "\n",
    "#             decoded, received_power = model(data_in,h1,h2,D,g1,g2,E0Nb_train)\n",
    "            decoded, y, noise = model(data_in,h1,h2,D,g1,g2,SNR_linear)\n",
    "#             received_power = sum(torch.linalg.norm(y,dim=1))\n",
    "#             received_power = (received_power/batch/Block_length)\n",
    "            loss = loss_fn(decoded, data_in)\n",
    "\n",
    "            optimizer.zero_grad()               # clear gradients for this training step\n",
    "            loss.backward()                     # backpropagation, compute gradients\n",
    "            optimizer.step()  \n",
    "            decode_out = decoded.cpu()\n",
    "            decode_out = decode_out.data.numpy()\n",
    "\n",
    "            position = np.argmax(decode_out,axis=1)\n",
    "            error = label.data.numpy() != position\n",
    "            num_error = error.astype(int).sum()\n",
    "            Ber = num_error/(batch*Block_length)\n",
    "            if step % 1 == 0:\n",
    "#                 print('Epoch: ',epoch, '|train loss: %.4f' %(loss.data),' BER: %.5f' %Ber, 'received_power: %3f' %received_power)\n",
    "                print('Epoch: ',epoch, '|train loss: %.4f' %(loss.data),' BER: %.5f' %Ber)\n",
    "            \n",
    "t1 = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'Autoencoder_BLength20_16QAM_Rice1_Scatter3_MIMO16x16_train15dB.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield x\n",
    "        x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "test_num = 1000\n",
    "test_label = (torch.rand([test_num,Block_length]) * N_bit).long()\n",
    "test_data = torch.zeros(test_num,N_bit,Block_length)\n",
    "channel_error = 0.0\n",
    "scale = 1e3\n",
    "for i in range(test_num):\n",
    "    test_data[i,:] = torch.transpose(torch.sparse.torch.eye(N_bit).index_select(dim=0, index=test_label[i,:]),0,1)\n",
    "    \n",
    "h1 = h1_train[:test_num,:]\n",
    "h2 = h2_train[:test_num,:]\n",
    "D = D_train[:test_num,:]\n",
    "g1 = g1_train[:test_num,:]\n",
    "g2 = g2_train[:test_num,:]\n",
    "\n",
    "\n",
    "EbNodB_range = list(frange(-10,21,2))\n",
    "Ber = torch.zeros(len(EbNodB_range))\n",
    "test_label = test_label.data.numpy()\n",
    "test_data = test_data.to(device)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    h1 = h1.to(device)\n",
    "    h2 = h2.to(device)\n",
    "    D = D.to(device)\n",
    "    g1 = g1.to(device)\n",
    "    g2 = g2.to(device)\n",
    "    \n",
    "    h1_tensor = torch.permute(h1,(0,3,1,2))\n",
    "    h1_tensor = torch.reshape(h1_tensor,(test_num*Block_length,N_RIS1,Nt))\n",
    "    \n",
    "    h2_tensor = torch.permute(h2,(0,3,1,2))\n",
    "    h2_tensor = torch.reshape(h2_tensor,(test_num*Block_length,N_RIS2,Nt))\n",
    "    \n",
    "    D_tensor = torch.permute(D,(0,3,1,2))\n",
    "    D_tensor = torch.reshape(D_tensor,(test_num*Block_length,N_RIS2,N_RIS1))\n",
    "    \n",
    "    g1_tensor = torch.permute(g1,(0,3,1,2))\n",
    "    g1_tensor = torch.reshape(g1_tensor,(test_num*Block_length,Nr,N_RIS1))\n",
    "    \n",
    "    g2_tensor = torch.permute(g2,(0,3,1,2))\n",
    "    g2_tensor = torch.reshape(g2_tensor,(test_num*Block_length,Nr,N_RIS2))\n",
    "    \n",
    "    encode_in = test_data\n",
    "    encode_out = model.encoder(encode_in)\n",
    "    mean = (encode_out**2).mean()\n",
    "    encode_out = encode_out / torch.sqrt(2*Nt*mean)\n",
    "    \n",
    "    encode_out_compl = torch.zeros([test_num,Nt,Block_length],dtype=torch.cfloat,device=device)\n",
    "    for i in range(Nt):\n",
    "        encode_out_compl[:,i,:] = torch.complex(encode_out[:,2*i,:],encode_out[:,2*i+1,:])\n",
    "    \n",
    "    #input for RIS1 \n",
    "    encode_out_compl_tensor = torch.permute(encode_out_compl,(0,2,1))\n",
    "    encode_out_compl_tensor = torch.reshape(encode_out_compl_tensor,(test_num*Block_length,Nt))\n",
    "        \n",
    "    y_ris1 = torch.matmul(h1_tensor,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "    y_ris1 = y_ris1*scale\n",
    "    temp = torch.reshape(torch.view_as_real(y_ris1),(test_num*Block_length,2*N_RIS1))\n",
    "    temp = torch.reshape(temp,(test_num,Block_length,2*N_RIS1))\n",
    "    y_ris1_real = torch.permute(temp,(0,2,1))\n",
    "        \n",
    "    phase1 = model.RIS1(y_ris1_real)\n",
    "#     phase = torch.randn(test_num,N_RIS,Block_length,device=device)\n",
    "    theta1 = torch.exp(1j*2*math.pi*phase1)\n",
    "    theta1_tensor = torch.permute(theta1,(0,2,1))\n",
    "    theta1_tensor = torch.reshape(theta1_tensor,(test_num*Block_length,N_RIS1))\n",
    "    \n",
    "    #input for RIS2\n",
    "    temp = torch.matmul(torch.diag_embed(theta1_tensor),h1_tensor)\n",
    "    H1phi1D_tensor = torch.matmul(D_tensor,temp)\n",
    "    y_ris2 = torch.matmul(H1phi1D_tensor*scale+h2_tensor*scale,encode_out_compl_tensor.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    temp = torch.reshape(torch.view_as_real(y_ris2),(test_num*Block_length,2*N_RIS2))\n",
    "    temp = torch.reshape(temp,(test_num,Block_length,2*N_RIS2))\n",
    "    y_ris2_real = torch.permute(temp,(0,2,1))\n",
    "  \n",
    "    phase2 = model.RIS2(y_ris2_real)\n",
    "    theta2 = torch.exp(1j*2*math.pi*phase2)\n",
    "      \n",
    "    theta2_tensor = torch.permute(theta2,(0,2,1))\n",
    "    theta2_tensor = torch.reshape(theta2_tensor,(test_num*Block_length,N_RIS2))\n",
    "    \n",
    "#calculate cascaded channel\n",
    "    temp = torch.matmul(torch.diag_embed(theta2_tensor),H1phi1D_tensor)\n",
    "    doublelink_tensor = torch.matmul(g2_tensor,temp)\n",
    "    \n",
    "    temp = torch.matmul(torch.diag_embed(theta1_tensor),h1_tensor)\n",
    "    H1phi1G1_tensor = torch.matmul(g1_tensor,temp)\n",
    "    \n",
    "    temp = torch.matmul(torch.diag_embed(theta2_tensor),h2_tensor)\n",
    "    H2phi2G2_tensor = torch.matmul(g2_tensor,temp)\n",
    "    \n",
    "    cascaded_channel_tensor = doublelink_tensor*scale**2 + H1phi1G1_tensor*scale**2 + H2phi2G2_tensor*scale**2\n",
    "    temp = torch.reshape(torch.view_as_real(cascaded_channel_tensor),(test_num*Block_length,2*Nr*Nt))\n",
    "    temp = torch.reshape(temp,(test_num,Block_length,2*Nr*Nt))\n",
    "    cascaded_channel_real = torch.permute(temp,(0,2,1))\n",
    "    noise = (torch.randn([test_num,2*Nt*Nr,Block_length],device=device)/math.sqrt(2))*channel_error\n",
    "    cascaded_channel_real = cascaded_channel_real \n",
    "    \n",
    "    decode_in, y_compl = model.AWGN(encode_out,h1_tensor, h2_tensor, D_tensor, g1_tensor, g2_tensor, phase1, phase2, EbNodB_range[n],device)\n",
    "   \n",
    "    \n",
    "    # mean = (decode_in**2).mean()\n",
    "    # decode_in = decode_in/torch.sqrt(2*mean)\n",
    "#     channel_mean = (cascaded_channel_real**2).mean()\n",
    "#     cascaded_channel_real = cascaded_channel_real/torch.sqrt(2*channel_mean)\n",
    "    noise_data = torch.randn(decode_in.shape)\n",
    "    decode_in_concat = torch.cat((decode_in,cascaded_channel_real),dim=1)\n",
    "    decode_out = model.decoder(decode_in_concat)\n",
    "    decode_out = decode_out.cpu()\n",
    "    decode_out = decode_out.data.numpy()\n",
    "    \n",
    "    position = np.argmax(decode_out,axis=1)\n",
    "    error = test_label != position\n",
    "    num_error = error.astype(int).sum()\n",
    "    Ber[n] += num_error/(test_num*Block_length)\n",
    "    \n",
    "    print(\"SNR {}: Ber = {}\".format(EbNodB_range[n],Ber[n]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(EbNodB_range, Ber, 'bo',label='Autoencoder(4,4)')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR Range')\n",
    "plt.ylabel('Block Error Rate')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Autoencoder_BLength20_16QAM_Rice1_Scatter3_MIMO16x16_train10dB.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = Ber.tolist()\n",
    "from scipy.io import savemat\n",
    "mdic = {\"ser\": ser}\n",
    "savemat(\"SER_Auto_Blength20_16x16_64RIS_64QAM_train20dB_3Scatter_Rice.2.mat\",mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
